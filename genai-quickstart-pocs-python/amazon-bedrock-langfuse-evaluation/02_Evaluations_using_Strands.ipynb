{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19165743",
   "metadata": {},
   "source": [
    "# Evaluating Bedrock LLM Solutions using AWS Strands, RAGAS and Langfuse \n",
    "\n",
    "This notebook demonstrates how to build an agent with observability and evaluation capabilities. \n",
    "\n",
    "We use [Langfuse](https://langfuse.com/) to process the Strands Agent traces and LLM as a judge to evaluate agent performance. The primary focus is on agent evaluation and the quality of responses generated by the agent using traces produced by the SDK.\n",
    "\n",
    "### What is Observability and Evaluation?\n",
    "\n",
    "**Observability** means being able to see what your AI agent is doing \"behind the scenes\" - like watching its thought process. It helps you understand why your agent makes certain decisions or gives particular responses.\n",
    "\n",
    "**Evaluation** is how we measure if our agent is doing a good job. Instead of just guessing if responses are good, we use specific metrics to score the agent's performance.\n",
    "\n",
    "### OpenTelemetry Integration\n",
    "\n",
    "Strands natively integrates with OpenTelemetry, an industry standard for distributed tracing. You can visualize and analyze traces using any OpenTelemetry-compatible tool. This integration provides:\n",
    "\n",
    "- **Compatibility with existing observability tools:** Send traces to platforms such as Jaeger, Grafana Tempo, AWS X-Ray, Datadog, and more\n",
    "- **Standardized attribute naming:** Uses OpenTelemetry semantic conventions\n",
    "- **Flexible export options:** Console output for development, OTLP endpoint for production\n",
    "- **Auto-instrumentation:** Trace creation is handled automatically when you turn on tracing\n",
    "\n",
    "### What are we Evaluating?\n",
    "\n",
    "In this Example we will be evaluating the knowledge base setup in notebook 01_Setup_S3_Vector_KnowledgeBase.ipynb.\n",
    "\n",
    "The knowledge base contains 10-k documents that have been added to an S3 Vector KnowledgeBase in Bedrock. We will build a test agent that will run through test scenarios in test_cases.json and try and extract usefull insights from the knowledgebase. \n",
    "\n",
    "The Agent will be configured to send all of its traces to langfuse. We will then pull these traces from Langfuse and evaluate them against metrics we have created using the RAGAS framework. We will then send these results back to langfuse where they can be reviewed and analysed. \n",
    "\n",
    "Lets get started.\n",
    "\n",
    "### Pre-Requisites\n",
    "\n",
    "- Run Jupyter Notebook 01_Setup_S3_Vector_KnowledgeBase.ipynb and Copy the created Knowledge Base ID\n",
    "\n",
    "- Create a langfuse account and project and copy the secret and public Keys. https://langfuse.com/docs/observability/get-started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e31401",
   "metadata": {},
   "source": [
    "### Install Required Packages\n",
    "\n",
    "First, we need to install all the necessary packages for our notebook. Each package has a specific purpose:\n",
    "\n",
    "- **langfuse**: Provides observability for our agent\n",
    "- **boto3**: AWS SDK for Python, used to access AWS services and Use Amazon Bedrock Models\n",
    "- **strands**: Framework for building AI agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc9ef0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21c842c",
   "metadata": {},
   "source": [
    "### Set Up and Configuration\n",
    "\n",
    "- Import Libraries \n",
    "- Set Environment Variables\n",
    "- Set up conection with Langfuse to send open telemetry data. \n",
    "- To get the Knowledge Base ID, you will need to run the steps in 01_Setup_S3_Vector_KnowledgeBase.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee1ed7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands import Agent\n",
    "from strands_tools import calculator\n",
    "from strands.tools import tool\n",
    "from langfuse import Langfuse\n",
    "from strands import Agent\n",
    "import json\n",
    "import boto3\n",
    "import uuid\n",
    "from utils import fetch_traces, process_traces, save_results_to_csv\n",
    "\n",
    "knowledge_base_id = \"YOUR_KNOWLEDGE_BASE_ID\"\n",
    "region_name = \"us-east-1\"\n",
    "\n",
    "# Initialize LangFuse client\n",
    "langfuse = Langfuse(\n",
    "    secret_key=\"your-langfuse-secret-key\",\n",
    "    public_key=\"your-langfuse-public-key\",\n",
    "    #host = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n",
    "    host = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100d2030",
   "metadata": {},
   "source": [
    " ### Create Knowledge Base Serach tool to add to the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffe5df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create tool to search knowledge base\n",
    "@tool\n",
    "def search_vector_db(query: str, customer_id: str) -> str:    \n",
    "    \"\"\"    \n",
    "    Handle document-based, narrative, and conceptual queries using the unstructured knowledge base.    \n",
    "    Args:        \n",
    "        query: A question about business strategies, policies, company information,or requiring document comprehension and qualitative analysis        \n",
    "        customer_id: Customer identifier    \n",
    "    Returns:        \n",
    "    Formatted string response from the knowledge base    \n",
    "    \"\"\"\n",
    "    kb_id = knowledge_base_id \n",
    "    bedrock_agent_runtime = boto3.client(\"bedrock-agent-runtime\", region_name=region_name)    \n",
    "    try:        \n",
    "        retrieve_response = bedrock_agent_runtime.retrieve(            \n",
    "            knowledgeBaseId=kb_id,            \n",
    "            retrievalQuery={\"text\": query},            \n",
    "            retrievalConfiguration={                \n",
    "                \"vectorSearchConfiguration\": {                    \n",
    "                    \"numberOfResults\": 5\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Format the response for better readability        \n",
    "        results = []        \n",
    "        for result in retrieve_response.get('retrievalResults', []):    \n",
    "            content = result.get('content', {}).get('text', '')  \n",
    "            \n",
    "        if content:                \n",
    "            results.append(content) \n",
    "        \n",
    "        return \"\\n\\n\".join(results) if results else \"No relevant information found.\"    \n",
    "    except Exception as e:        \n",
    "        return f\"Error in unstructured data assistant: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a703fcd",
   "metadata": {},
   "source": [
    "### Create an Agent tha we would like to Evaluate. \n",
    "\n",
    "This Agent is going to Analyze 10-k documents and provide responses to questions from uses about companies. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f888411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an evaluator agent with a stronger model\n",
    "test_agent = Agent(\n",
    "    #model=\"us.amazon.nova-lite-v1:0\",\n",
    "    model=\"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "    tools=[search_vector_db, calculator],\n",
    "    system_prompt=\"\"\"\n",
    "        You are an Finacial Analyst. Your job is to prvide detail analytical responses based on 10-k documents.\n",
    "        You will look up data from the knowledge base and use the tools to answer questions. \n",
    "        If you are not able to answer the question you will say so. \n",
    "    \"\"\",\n",
    "    record_direct_tool_call = True,  # Record when tools are used\n",
    "    trace_attributes={\n",
    "        \"session.id\": str(uuid.uuid4()),  # Generate a unique session ID\n",
    "        \"user.id\": \"henry.j.a.lee@gmail.com\",  # Example user ID\n",
    "        \"langfuse.tags\": [\n",
    "            \"Agent-SDK-Example\",\n",
    "            \"Strands-Project-Demo\",\n",
    "            \"Observability-Tutorial\"\n",
    "        ],\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58079ef4",
   "metadata": {},
   "source": [
    "### Now we can run the Agent against some test cases with expected results built by human analysts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daa261b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an evaluator agent with a stronger model\n",
    "evaluator = Agent(\n",
    "    model=\"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "    system_prompt=\"\"\"\n",
    "    You are an expert AI evaluator. Your job is to assess the quality of AI responses based on:\n",
    "    1. Accuracy - factual correctness of the response\n",
    "    2. Relevance - how well the response addresses the query\n",
    "    3. Completeness - whether all aspects of the query are addressed\n",
    "    4. Tool usage - appropriate use of available tools\n",
    "\n",
    "    Score each criterion from 1-5, where 1 is poor and 5 is excellent.\n",
    "    Provide an overall score and brief explanation for your assessment.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Load test cases\n",
    "with open(\"test_cases.json\", \"r\") as f:\n",
    "    test_cases = json.load(f)\n",
    "\n",
    "for case in test_cases[\"questions\"]:\n",
    "    # Get agent response\n",
    "    agent_response = test_agent(case[\"query\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e9984e",
   "metadata": {},
   "source": [
    "### Define RAGAS Metrics\n",
    "\n",
    "We'll define several metrics to evaluate different aspects of our agent's performance:\n",
    "\n",
    "We are going to look at the following criteria:\n",
    "\n",
    "    Request Completeness - Has the LLM fullfilled the users request\n",
    "    Brand Voice - Has the LLM responded in a polite and professional manner\n",
    "    Tool Usage - Did the LLM use the tools available to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e5073e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import ragas Libraries and Bedrock Model for Evaluation\n",
    "from ragas.metrics import AspectCritic\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "#Set Bedrock Model for evaluation \n",
    "model = ChatBedrock(model_id='us.anthropic.claude-sonnet-4-20250514-v1:0', region_name=region_name)\n",
    "\n",
    "# Set up the evaluator LLM (we'll use the same model as our agent)\n",
    "evaluator_llm = LangchainLLMWrapper(model)\n",
    "\n",
    "# Metric to check if the agent fulfills all user requests\n",
    "request_completeness = AspectCritic(\n",
    "    name=\"Request Completeness\",\n",
    "    llm=evaluator_llm,\n",
    "    definition=(\n",
    "        \"Return 1 if the agent completely fulfills all the user requests with no omissions. \"\n",
    "        \"otherwise, return 0.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Metric to assess if the AI's communication aligns with the desired brand voice\n",
    "brand_tone = AspectCritic(\n",
    "    name=\"Brand Voice Metric\",\n",
    "    llm=evaluator_llm,\n",
    "    definition=(\n",
    "        \"Return 1 if the AI's communication is friendly, approachable, helpful, clear, and concise; \"\n",
    "        \"otherwise, return 0.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Tool usage effectiveness metric\n",
    "tool_usage_effectiveness = AspectCritic(\n",
    "    name=\"Tool Usage Effectiveness\",\n",
    "    llm=evaluator_llm,\n",
    "    definition=(\n",
    "        \"Return 1 if the agent appropriately used available tools to fulfill the user's request \"\n",
    "        \"(such as using search_vector_db for general questions and calculator for financial calculations). \"\n",
    "        \"Return 0 if the agent failed to use appropriate tools or used unnecessary tools.\"\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bd7577",
   "metadata": {},
   "source": [
    "### Create a rubric to help the evaluation model score the responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f36f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import RubricsScore\n",
    "\n",
    "# Define a rubric for evaluating recommendations\n",
    "rubrics = {\n",
    "    \"score1_description\": (\n",
    "        \"\"\"The data required to answer the question is not present in the knowledge base, but the model provides \n",
    "        incorrect hallucinated or made up data to answer the question\"\"\"\n",
    "    ),\n",
    "    \"score2_description\": (\n",
    "        \"\"\"The data required to answer the question is not present in the knowledge base \n",
    "        and the model explains that it does not have enoough information to answer the question accurately\"\"\"\n",
    "    ),\n",
    "    \"score3_description\": (\n",
    "        \"The model retrieves te correct data from the knowledge base \"\n",
    "        \"but is not able to do the calculations needed to provide and accurate answer\"\n",
    "    ),\n",
    "    \"score4_description\": (\n",
    "        \"The model retrieves the right data from the knowledge base \"\n",
    "        \"and does the calculations needed to provide an accurate answer\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Create the recommendations metric\n",
    "recommendations = RubricsScore(rubrics=rubrics, llm=evaluator_llm, name=\"Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c312fcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ragas.dataset_schema import (\n",
    "    EvaluationDataset\n",
    ")\n",
    "\n",
    "from ragas import evaluate\n",
    "\n",
    "\n",
    "def evaluate_conversation_samples(multi_turn_samples, trace_sample_mapping):\n",
    "    \"\"\"Evaluate conversation-based samples and push scores to Langfuse\"\"\"\n",
    "    if not multi_turn_samples:\n",
    "        print(\"No multi-turn samples to evaluate\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Evaluating {len(multi_turn_samples)} multi-turn samples with conversation metrics\")\n",
    "    conv_dataset = EvaluationDataset(samples=multi_turn_samples)\n",
    "    conv_results = evaluate(\n",
    "        dataset=conv_dataset,\n",
    "        metrics=[\n",
    "            request_completeness, \n",
    "            recommendations,\n",
    "            brand_tone,\n",
    "            tool_usage_effectiveness\n",
    "        ]\n",
    "        \n",
    "    )\n",
    "    conv_df = conv_results.to_pandas()\n",
    "    \n",
    "    # Push conversation scores back to Langfuse\n",
    "    for mapping in trace_sample_mapping:\n",
    "        if mapping[\"type\"] == \"multi_turn\":\n",
    "            sample_index = mapping[\"index\"]\n",
    "            trace_id = mapping[\"trace_id\"]\n",
    "            \n",
    "            if sample_index < len(conv_df):\n",
    "                for metric_name in conv_df.columns:\n",
    "                    if metric_name not in ['user_input']:\n",
    "                        try:\n",
    "                            metric_value = float(conv_df.iloc[sample_index][metric_name])\n",
    "                            if pd.isna(metric_value):\n",
    "                                metric_value = 0.0\n",
    "                            langfuse.create_score(\n",
    "                                trace_id=trace_id,\n",
    "                                name=metric_name,\n",
    "                                value=metric_value\n",
    "                            )\n",
    "                            print(f\"Added score {metric_name}={metric_value} to trace {trace_id}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error adding conversation score: {e}\")\n",
    "    \n",
    "    return conv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd5608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_traces(batch_size=10, lookback_hours=24, tags=None, save_csv=False):\n",
    "    \"\"\"Main function to fetch traces, evaluate them with RAGAS, and push scores back to Langfuse\"\"\"\n",
    "    # Fetch traces from Langfuse\n",
    "    traces = fetch_traces(langfuse, batch_size, lookback_hours, tags)\n",
    "    \n",
    "    if not traces:\n",
    "        print(\"No traces found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Process traces into samples\n",
    "    processed_data = process_traces(langfuse, traces)\n",
    "\n",
    "    \n",
    "    conv_df = evaluate_conversation_samples(\n",
    "        processed_data[\"multi_turn_samples\"], \n",
    "        processed_data[\"trace_sample_mapping\"]\n",
    "    )\n",
    "    \n",
    "    # Save results to CSV if requested\n",
    "    if save_csv:\n",
    "        save_results_to_csv(conv_df)\n",
    "    \n",
    "    return {\n",
    "        \"conversation_results\": conv_df\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b53294",
   "metadata": {},
   "source": [
    "### Now lets pull the traces from langfuse for evaluation and push the results back. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75ca8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_traces(\n",
    "    lookback_hours=2,\n",
    "    batch_size=20,\n",
    "    tags=[\"Agent-SDK-Example\"],\n",
    "    save_csv=True\n",
    ")\n",
    "\n",
    "# Access results if needed for further analysis\n",
    "if results:\n",
    "    if \"conversation_results\" in results and results[\"conversation_results\"] is not None:\n",
    "        print(\"\\nConversation Evaluation Summary:\")\n",
    "        print(results[\"conversation_results\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76050d5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa198ac4",
   "metadata": {},
   "source": [
    "### Check results and Clean up resources\n",
    "\n",
    "You should now be able to review the results of the evaluation in Langfuse. You should be able to see through the aalysis score that not all the questions were correctly answered, which shows that the 10-k Documents may need additional processeing before being add to the knowledge base or that the financial data would be better sourced from a structured data source. \n",
    "\n",
    "To clean up the resources you should go back to 01_Setup_S3_Vector_KnowledgeBase.ipynb and run the clean up section with the clean_resources flag changed to True. \n",
    "\n",
    "Thanks!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
